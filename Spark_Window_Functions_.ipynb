{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark Window Functions .ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vamsi1905/ARGUS/blob/master/Spark_Window_Functions_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5wDgv2wKP4n",
        "outputId": "2a54f50e-7523-462d-8bc4-4bf87acde5f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 33 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 60.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=02596574e89ba0d6cff8958bdb1ee69b5d8e280de66cd8f678ebd3f5a3b60f42\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "import re\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "  \n",
        "spark = SparkSession.builder.appName(\"Spark Window Functions \").getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "C_rzbBtfKaQD",
        "outputId": "c08534d7-2191-4845-9060-2b1a52d2e814"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7ff725423a10>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://6fa71396eace:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Spark Window Functions </code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
        "data1 = [(\"Jame{\",\"\",\"Smith\",\"36636\",\"M\", 1000, \"Sales\", 2020),\n",
        "    (\"MichaeA\",\"Rose\",\"\",\"40288\",\"M\", 2000, \"Operations\",2020),\n",
        "    (\"RoberB\",\"\",\"Williams\",\"42114\",\"M\", 3000, \"Sales\",2020),\n",
        "    (\"MariC\",\"Anne\",\"Jones\",\"39192\",\"F\", 4000, \"Operations\",2020),\n",
        "    (\"JameD\",\"\",\"Smith\",\"36636\",\"M\", 1050, \"Sales\", 2021),\n",
        "    (\"MichaeR\",\"Rose\",\"\",\"40288\",\"M\", 1950, \"Operations\",2021),\n",
        "    (\"RoberJ\",\"\",\"Williams\",\"42114\",\"M\", 3100, \"Sales\",2021),\n",
        "    (\"MariA\",\"Anne\",\"Jones\",\"39192\",\"F\", 4200, \"Operations\",2021),\n",
        "     (\"JameE\",\"\",\"Smith\",\"36636\",\"M\", 1050, \"Sales\", 2017),\n",
        "    (\"MichaeF\",\"Rose\",\"\",\"40288\",\"M\", 1950, \"Operations\",2017),\n",
        "    (\"RoberG\",\"\",\"Williams\",\"42114\",\"M\", 3100, \"Sales\",2017),\n",
        "    (\"MariH\",\"Anne\",\"Jones\",\"39192\",\"F\", 4200, \"Operations\",2017),\n",
        "    (\"JameI\",\"\",\"Smith\",\"36636\",\"M\", 990, \"Sales\", 2019),\n",
        "    (\"MichaeC\",\"Rose\",\"\",\"40288\",\"M\", 2200, \"Operations\",2019),\n",
        "    (\"RoberA\",\"\",\"Williams\",\"42114\",\"M\", 2900, \"Sales\",2019),\n",
        "    (\"MariaA\",\"Anne\",\"Jones\",\"39192\",\"F\", 4500, \"Operations\",2019)\n",
        "\n",
        " \n",
        "  \n",
        "  ]\n",
        "\n",
        "schema1 = StructType([ \\\n",
        "    StructField(\"firstname\",StringType(),True), \\\n",
        "    StructField(\"middlename\",StringType(),True), \\\n",
        "    StructField(\"lastname\",StringType(),True), \\\n",
        "    StructField(\"id\", StringType(), True), \\\n",
        "    StructField(\"gender\", StringType(), True),\n",
        "    StructField(\"annualsalary\", IntegerType(), True),\n",
        "    StructField(\"work\", StringType(), True),\n",
        "    StructField(\"year\", IntegerType(), True),\n",
        "   \n",
        "  ])\n",
        " \n",
        "df1 = spark.createDataFrame(data=data1,schema=schema1)\n",
        "df1.printSchema()\n",
        "df1.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fWa75d-Mxdm",
        "outputId": "f06a8ba8-ab34-4ea5-cff5-56ce1bcfda03"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- firstname: string (nullable = true)\n",
            " |-- middlename: string (nullable = true)\n",
            " |-- lastname: string (nullable = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- annualsalary: integer (nullable = true)\n",
            " |-- work: string (nullable = true)\n",
            " |-- year: integer (nullable = true)\n",
            "\n",
            "+---------+----------+--------+-----+------+------------+----------+----+\n",
            "|firstname|middlename|lastname|id   |gender|annualsalary|work      |year|\n",
            "+---------+----------+--------+-----+------+------------+----------+----+\n",
            "|Jame{    |          |Smith   |36636|M     |1000        |Sales     |2020|\n",
            "|MichaeA  |Rose      |        |40288|M     |2000        |Operations|2020|\n",
            "|RoberB   |          |Williams|42114|M     |3000        |Sales     |2020|\n",
            "|MariC    |Anne      |Jones   |39192|F     |4000        |Operations|2020|\n",
            "|JameD    |          |Smith   |36636|M     |1050        |Sales     |2021|\n",
            "|MichaeR  |Rose      |        |40288|M     |1950        |Operations|2021|\n",
            "|RoberJ   |          |Williams|42114|M     |3100        |Sales     |2021|\n",
            "|MariA    |Anne      |Jones   |39192|F     |4200        |Operations|2021|\n",
            "|JameE    |          |Smith   |36636|M     |1050        |Sales     |2017|\n",
            "|MichaeF  |Rose      |        |40288|M     |1950        |Operations|2017|\n",
            "|RoberG   |          |Williams|42114|M     |3100        |Sales     |2017|\n",
            "|MariH    |Anne      |Jones   |39192|F     |4200        |Operations|2017|\n",
            "|JameI    |          |Smith   |36636|M     |990         |Sales     |2019|\n",
            "|MichaeC  |Rose      |        |40288|M     |2200        |Operations|2019|\n",
            "|RoberA   |          |Williams|42114|M     |2900        |Sales     |2019|\n",
            "|MariaA   |Anne      |Jones   |39192|F     |4500        |Operations|2019|\n",
            "+---------+----------+--------+-----+------+------------+----------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number\n",
        "windowSpec  = Window.partitionBy(\"gender\",\"work\").orderBy(\"annualsalary\")"
      ],
      "metadata": {
        "id": "Wxh9YXhcfQzv"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ttECAAxmunFv"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "udf_replace_non_ascii_mapping = udf(lambda x:replace_non_ascii_mapping(x),StringType())\n",
        "spark.udf.register(\"udf_replace_non_ascii_mapping\", udf_replace_non_ascii_mapping)"
      ],
      "metadata": {
        "id": "VsaPxdGJA_Ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_non_ascii_mapping(col_string, key, value):\n",
        "  col_string = re.sub(key, value, col_string)\n",
        "  return col_string\n",
        "\n",
        "def replace_non_ascii(df, col_name):\n",
        "  non_ascii_mapping = { \n",
        "                        \"{\":\"0\",\t\t\n",
        "                        \"A\":\"1\",\n",
        "                        \"B\":\"2\",\n",
        "                        \"C\":\"3\",\n",
        "                        \"D\":\"4\",\n",
        "                        \"E\":\"5\",\n",
        "                        \"F\":\"6\",\n",
        "                        \"G\":\"7\",\n",
        "                        \"H\":\"8\",\n",
        "                        \"I\": \"9\",\n",
        "                        \"}\":\"0\",\n",
        "                        \"J\":\"1\",\n",
        "                        \"K\":\"2\",\n",
        "                        \"L\":\"3\",\n",
        "                        \"M\":\"4\",\n",
        "                        \"N\":\"5\",\n",
        "                        \"O\":\"6\",\n",
        "                        \"P\":\"7\",\n",
        "                        \"Q\":\"8\",\n",
        "                        \"R\": \"9\"\n",
        "                        }\n",
        "  for key, value in non_ascii_mapping.items():\n",
        "    if df.filter(F.col(col_name).contains(key)):\n",
        "      df.withColumn(col_name, udf_replace_non_ascii_mapping(F.col(col_name)))\n",
        "  return df"
      ],
      "metadata": {
        "id": "BFQJnVU2pvtE"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf,col\n",
        "from pyspark.sql.types import StringType\n"
      ],
      "metadata": {
        "id": "5Fjy5m3Srbau"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = "
      ],
      "metadata": {
        "id": "c_66QLkErzB7",
        "outputId": "4cd20625-abc0-4a2c-d1fc-66e9de02ac8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+\n",
            "|firstname|lastname|\n",
            "+---------+--------+\n",
            "|    1ame0|   Smith|\n",
            "|  4ichae1|        |\n",
            "|   9ober2|Williams|\n",
            "|    4ari3|   Jones|\n",
            "|    1ame4|   Smith|\n",
            "|  4ichae9|        |\n",
            "|   9ober1|Williams|\n",
            "|    4ari1|   Jones|\n",
            "|    1ame5|   Smith|\n",
            "|  4ichae6|        |\n",
            "|   9ober7|Williams|\n",
            "|    4ari8|   Jones|\n",
            "|    1ame9|   Smith|\n",
            "|  4ichae3|        |\n",
            "|   9ober1|Williams|\n",
            "|   4aria1|   Jones|\n",
            "+---------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rank Function "
      ],
      "metadata": {
        "id": "X9nIZMSXfBuy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "rank() window function is used to provide a rank to the result within a window partition. This function leaves gaps in rank when there are ties."
      ],
      "metadata": {
        "id": "V5jnht9CfHyk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fTvOHexQgMbb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The difference between **rank** and **dense_rank** is that denseRank leaves no gaps in ranking sequence when there are ties. That is, if you were ranking a competition using dense_rank and had three people tie for second place, you would say that all three were in second place and that the next person came in third. Rank would give me sequential numbers, making the person that came in third place (after the ties) would register as coming in fifth."
      ],
      "metadata": {
        "id": "TlowIeDdgMOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Gb5_5W-IgYCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import rank\n",
        "df1.withColumn(\"rank\",rank().over(windowSpec)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXuaYpu-fFbp",
        "outputId": "ff562cc2-fd39-4344-954f-9c7002370920"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+--------+-----+------+------------+----------+----+----+\n",
            "|firstname|middlename|lastname|   id|gender|annualsalary|      work|year|rank|\n",
            "+---------+----------+--------+-----+------+------------+----------+----+----+\n",
            "|    Maria|      Anne|   Jones|39192|     F|        4000|Operations|2020|   1|\n",
            "|    Maria|      Anne|   Jones|39192|     F|        4200|Operations|2021|   2|\n",
            "|    Maria|      Anne|   Jones|39192|     F|        4200|Operations|2017|   2|\n",
            "|    Maria|      Anne|   Jones|39192|     F|        4500|Operations|2019|   4|\n",
            "|  Michael|      Rose|        |40288|     M|        1950|Operations|2021|   1|\n",
            "|  Michael|      Rose|        |40288|     M|        1950|Operations|2017|   1|\n",
            "|  Michael|      Rose|        |40288|     M|        2000|Operations|2020|   3|\n",
            "|  Michael|      Rose|        |40288|     M|        2200|Operations|2019|   4|\n",
            "|    James|          |   Smith|36636|     M|         990|     Sales|2019|   1|\n",
            "|    James|          |   Smith|36636|     M|        1000|     Sales|2020|   2|\n",
            "|    James|          |   Smith|36636|     M|        1050|     Sales|2021|   3|\n",
            "|    James|          |   Smith|36636|     M|        1050|     Sales|2017|   3|\n",
            "|   Robert|          |Williams|42114|     M|        2900|     Sales|2019|   5|\n",
            "|   Robert|          |Williams|42114|     M|        3000|     Sales|2020|   6|\n",
            "|   Robert|          |Williams|42114|     M|        3100|     Sales|2021|   7|\n",
            "|   Robert|          |Williams|42114|     M|        3100|     Sales|2017|   7|\n",
            "+---------+----------+--------+-----+------+------------+----------+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import dense_rank\n",
        "df1.withColumn(\"dense_rank\",dense_rank().over(windowSpec)).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqga9ySigluF",
        "outputId": "4da59adb-dbbb-47a3-9c9b-fe20bb3ce6fe"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+--------+-----+------+------------+----------+----+----------+\n",
            "|firstname|middlename|lastname|   id|gender|annualsalary|      work|year|dense_rank|\n",
            "+---------+----------+--------+-----+------+------------+----------+----+----------+\n",
            "|    Maria|      Anne|   Jones|39192|     F|        4000|Operations|2020|         1|\n",
            "|    Maria|      Anne|   Jones|39192|     F|        4200|Operations|2021|         2|\n",
            "|    Maria|      Anne|   Jones|39192|     F|        4200|Operations|2017|         2|\n",
            "|    Maria|      Anne|   Jones|39192|     F|        4500|Operations|2019|         3|\n",
            "|  Michael|      Rose|        |40288|     M|        1950|Operations|2021|         1|\n",
            "|  Michael|      Rose|        |40288|     M|        1950|Operations|2017|         1|\n",
            "|  Michael|      Rose|        |40288|     M|        2000|Operations|2020|         2|\n",
            "|  Michael|      Rose|        |40288|     M|        2200|Operations|2019|         3|\n",
            "|    James|          |   Smith|36636|     M|         990|     Sales|2019|         1|\n",
            "|    James|          |   Smith|36636|     M|        1000|     Sales|2020|         2|\n",
            "|    James|          |   Smith|36636|     M|        1050|     Sales|2021|         3|\n",
            "|    James|          |   Smith|36636|     M|        1050|     Sales|2017|         3|\n",
            "|   Robert|          |Williams|42114|     M|        2900|     Sales|2019|         4|\n",
            "|   Robert|          |Williams|42114|     M|        3000|     Sales|2020|         5|\n",
            "|   Robert|          |Williams|42114|     M|        3100|     Sales|2021|         6|\n",
            "|   Robert|          |Williams|42114|     M|        3100|     Sales|2017|         6|\n",
            "+---------+----------+--------+-----+------+------------+----------+----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lnFv3fiJglec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Percent Rank"
      ],
      "metadata": {
        "id": "sLUY2dyVfpkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import percent_rank\n",
        "df1.withColumn(\"percent_rank\",percent_rank().over(windowSpec)) \\\n",
        "    .show()"
      ],
      "metadata": {
        "id": "a2TbHSx4fsco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# row_number Window Function\n",
        "\n",
        "returns a sequential number starting at 1 within a window partition.\n"
      ],
      "metadata": {
        "id": "vqXCRQ4SdgDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "df1.withColumn(\"row_number\",row_number().over(windowSpec)) \\\n",
        "    .show(truncate=False)"
      ],
      "metadata": {
        "id": "N4_3iM6Bdtkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analytical Window Functions - Lag & Lead"
      ],
      "metadata": {
        "id": "2PONrFBFhk_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lag** Window function: returns the value that is `offset` rows before the current row, and 'null` if there is less than `offset` rows before the current row. For example, \n",
        "   * an `offset` of one will return the previous row at any given point in the window partition."
      ],
      "metadata": {
        "id": "c1KXaClPjFoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lag\n",
        "windowYear  = Window.partitionBy(\"id\").orderBy(\"year\")\n",
        " \n",
        "df1 = df1.withColumn(\"PreviousYearSalary\", lag(\"annualSalary\", 1).over(windowYear))\n",
        "df1.select(\"firstname\",\"year\",\"annualsalary\",\"PreviousYearSalary\").show()"
      ],
      "metadata": {
        "id": "ezr1oEkDhsPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "   **Lead ** Window function: returns the value that is `offset` rows after the current row, and `null` if there is less than `offset` rows after the current row. For example,  an `offset` of one will return the next row at any given point in the window partition."
      ],
      "metadata": {
        "id": "vKgL7zJSjPlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lead\n",
        "windowYear  = Window.partitionBy(\"id\").orderBy(\"year\")\n",
        " \n",
        "df1 = df1.withColumn(\"NextYearSalary\", lead(\"annualSalary\", 1).over(windowYear))\n",
        "df1.select(\"firstname\",\"year\",\"PreviousYearSalary\",\"annualsalary\",\"NextYearSalary\").show()"
      ],
      "metadata": {
        "id": "jpciCjB7it4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lag and Lead with negative offsets**\n",
        "\n"
      ],
      "metadata": {
        "id": "HvJbBwV3mihD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lag, lead\n",
        "windowYear  = Window.partitionBy(\"id\").orderBy(\"year\")\n",
        " \n",
        "df1 = df1.withColumn(\"salary2\", lead(\"annualSalary\", -1).over(windowYear)) \n",
        "df1 = df1.withColumn(\"salary1\", lag(\"annualSalary\", -1).over(windowYear))\n",
        "df1.select(\"firstname\",\"year\",\"annualsalary\",\"salary2\", \"PreviousYearSalary\",\"salary1\", \"NextYearSalary\").show()"
      ],
      "metadata": {
        "id": "UEoF2KrJmKqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source Code for reference :\n",
        "https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/functions.scala"
      ],
      "metadata": {
        "id": "QtM8wEx3jeav"
      }
    }
  ]
}